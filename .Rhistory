for (y in 1:loop) {
k = y * 1000
l = k -1000 + 1
if (k > NROW(data)){
k <- NROW(data)
}
data.subset <- data[l:k, ]
if (exists("vector.for.next.epoch")){
vector.for.next.epoch <- as.data.frame(vector.for.next.epoch)
names(vector.for.next.epoch) <- c(names(data.subset)[1], names(data.subset)[2])
data.subset <- rbind(data.subset, vector.for.next.epoch)
f.path <- paste0("C:/users/nikos/Desktop/ap.spark.data/to.be.clustered/dataset", y)
write.csv(data.subset, f.path)
}else{
f.path <- paste0("C:/users/nikos/Desktop/ap.spark.data/to.be.clustered/dataset", y)
write.csv(data.subset, f.path)
}
#estimation of similarity
sim <- negDistMat(data.subset, r = 2)
#affinity propagation
#p.value = round(sum(data.subset) / 2000)
#p.value
exemplar.cluster.0 <- apcluster(sim, details = TRUE, q = 0.0229)
exemplar.cluster.0
#storage of exemplars of the subset
exemplars.epoch.0 <- exemplar.cluster.0@exemplars
#creation of a dataframe that has the values, the exemplars and the subset indication
total.info <- as.data.frame(matrix(nrow = 0, ncol = 3))
names(total.info) <- c("values", "exemplar", "subset")
#appending in the dataframe values, exemplars and index
for(i in 1:NROW(exemplar.cluster.0@clusters)){
processed.subset <- as.data.frame(matrix(nrow = 0, ncol = 1))
names(processed.subset) <- c("values")
processed.subset <- rbind(processed.subset, exemplar.cluster.0@clusters[i])
processed.subset$exemplar <- exemplar.cluster.0@exemplars[i]
processed.subset$subset <- i
names(processed.subset) <- c("values", "exemplar", "subset")
names(total.info) <- c("values", "exemplar", "subset")
total.info <- rbind(total.info, processed.subset)
rm(processed.subset)
}
#just some calculations for adjusting alphabetical order between the shape files
wy = y
if (wy < 10){
wy <-paste0("0",wy)
}
file.to.stream.path <- paste("C:/users/nikos/Desktop/ap.spark.data/datasets/", wy, ".csv")
file.to.stream.path <- gsub(" ", "", file.to.stream.path)
write.csv(total.info, file.to.stream.path)
#creation of graph of the first subset
plot(exemplar.cluster.0, data.subset)
plot.file.path <- paste("C:/users/nikos/Desktop/ap.spark.data/graphs/", wy, ".png")
plot.file.path <- gsub(" ", "", plot.file.path)
plot.file.path
png(filename=plot.file.path)
plot(exemplar.cluster.0, data.subset)
dev.off()
#weighted.exemplars
t <- y - (y-1) - 1
w <- 2^(-0.25*(t))
weighted.exemplars <- w*exemplar.cluster.0@exemplars
#number of values clustered in each exemplar
c <- NA
for (i in 1:NROW(exemplar.cluster.0@clusters)){
d <- summary(exemplar.cluster.0@clusters[i])[1]
c <- rbind(c, d)
c <- c[-1]
}
c <- as.numeric(c)
c <- c* w
#c contains all the weights for the exemplars of the first 1000 rows
#cd contains all weights for each exemplar and each exemplar
cd <- cbind(c, exemplar.cluster.0@exemplars)
#a contains the weights multiplied by the exemplars
a <- as.numeric(cd[ ,1] * cd[ ,2])
#  a <- cd[ ,1] * cd[ ,2]
#calculate disimilarity for each exemplar's group of values
#calculation of the vector that is binded in the next subset of data
vector.for.next.epoch <- matrix(nrow = 0, ncol = 2)
for (i in 1:NROW(exemplar.cluster.0@clusters)){
f <- as.data.frame(exemplar.cluster.0@clusters[i])
sumD <- sum(-negDistMat(f, r = 2))
c <- as.array(a[i], sumD)
z <- cbind(a[i], sumD)
vector.for.next.epoch <- rbind(vector.for.next.epoch, z)
#filepath = paste("~/Desktop/ap.spark.data/vectors/ap.vector.for.next.epoch",
#                 y, ".csv")
#  filepath = gsub(" ", "", filepath)
# write.csv(vector.for.next.epoch, filepath)
}
qw = y
if (qw < 10){
qw <- paste0("0",qw)
}
filepath = paste("C:/users/nikos/Desktop/ap.spark.data/vectors/vector.for.next.epoch", qw, ".csv")
filepath = gsub(" ", "", filepath)
write.csv(vector.for.next.epoch, filepath)
end_time <- Sys.time()
time.taken <- end_time - start_time
total_time<- total_time + time.taken
cat("Time spent\n", time.taken, "\n" )
start_time=Sys.time()
end_time=Sys.time()
}
cat("Time spent for system\n", total_time, "\n" )
list <- as.data.frame(exemplar.cluster.0@exemplars)
list <- rownames(list)
cat("final exemplars are:", list)
data.1 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data1.txt", sep = "\t")
data.2 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data2.txt", sep = "\t")
data.3 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data3.txt", sep = "\t")
names(data.1) <- names(data.2)
names(data.3) <- names(data.2)
data <- rbind(data.1, data.2, data.3)
require(apcluster)
#graphs loop
for (y in 1:33){
k = y * 1000
l = k -1000 + 1
if (k > NROW(data)){
k <- NROW(data)
}
data.subset <- data[l:k, ]
sim <- negDistMat(data.subset, r = 2)
exemplar.cluster.0 <- apcluster(sim, details = TRUE, q = 0.0229)
#creation of graph of the first subset
plot(exemplar.cluster.0, data.subset)
plot.file.path <- paste("C:/users/nikos/Desktop/ap.spark.data/graphs.without.weights/", y, ".png")
plot.file.path <- gsub(" ", "", plot.file.path)
plot.file.path
png(filename=plot.file.path)
plot(exemplar.cluster.0, data.subset)
dev.off()
}
data.1 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data1.txt", sep = "\t")
data.2 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data2.txt", sep = "\t")
data.3 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data3.txt", sep = "\t")
names(data.1) <- names(data.2)
names(data.3) <- names(data.2)
View(data.1)
#import datasets
dataset.urls <- list.files (path = "C:/users/nikos/Desktop/ap.spark.data/datasets/")
dataset.urls <- paste0("C:/users/nikos/Desktop/ap.spark.data/datasets/", dataset.urls)
dataset.urls <- dataset.urls[1:33]
#import of dataset1 at dataset.1
dataset.1 <- read.csv(dataset.urls[1])
#import of dataset2 at dataset.2
dataset.2 <- read.csv(dataset.urls[2])
#NA fill in the 1st dataset
dataset.1$subset[dataset.1$exemplar == unique(dataset.1$exemplar)[1]] <- 1
dataset.1$subset[dataset.1$exemplar == unique(dataset.1$exemplar)[2]] <- 2
dataset.1$subset[dataset.1$exemplar == unique(dataset.1$exemplar)[3]] <- 3
#label set in previous dataset
dataset.1$subset[dataset.1$subset == 1] <- dataset.2$subset[1001]
dataset.1$subset[dataset.1$subset == 2] <- dataset.2$subset[1002]
dataset.1$subset[dataset.1$subset == 3] <- dataset.2$subset[1003]
#rbind all data up to epoch.2
dataset.1.2 <- rbind(dataset.1, dataset.2[1:1000, ])
dataset.total <- dataset.1.2
#from 2 to 3
dataset.1 <- dataset.1.2
dataset.2 <- read.csv(dataset.urls[3])
dataset.1$subset[dataset.1$subset == 1] <- dataset.2$subset[1001]
dataset.1$subset[dataset.1$subset == 2] <- dataset.2$subset[1002]
dataset.1$subset[dataset.1$subset == 3] <- dataset.2$subset[1003]
dataset.1$subset[dataset.1$subset == 4] <- dataset.2$subset[1004]
dataset.1$subset[dataset.1$subset == 5] <- dataset.2$subset[1005]
dataset.1$subset[dataset.1$subset == 6] <- dataset.2$subset[1006]
dataset.1.2.3 <- rbind(dataset.total, dataset.2[1:1000, ])
dataset.total <- dataset.1.2.3
#export data
write.csv(dataset.1.2, "C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.2.csv")
write.csv(dataset.1.2.3, "C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.2.3.csv")
#dataset.total <- dataset.1.2.3
#loop for epoch 4:6
for (j in 4:6){
dataset.1 <- dataset.total
dataset.2 <- read.csv(dataset.urls[j])
for (k in 1:NROW(unique(dataset.1$subset))) {
l <- as.integer(k + 1000)
k <- as.integer(k)
dataset.1$subset[dataset.1$subset == k] <- dataset.2$subset[l]
}
dataset.total <- rbind(dataset.1, dataset.2[1:1000, ])
}
#export  dataset 1 to 6
write.csv(dataset.total, "C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.6.csv")
#dataset.total <- read.csv("~/Desktop/ap.spark.data/final.clusters/dataset.1.6.csv")
#dataset.total <- dataset.total[ , -1]
#export dataset 7 to 9
for (j in 7:9){
dataset.1 <- dataset.total
dataset.2 <- read.csv(dataset.urls[j])
for (k in 1:NROW(unique(dataset.1$subset))) {
l <- as.integer(k + 1000)
k <- as.integer(k)
dataset.1$subset[dataset.1$subset == k] <- dataset.2$subset[l]
}
dataset.total <- rbind(dataset.1, dataset.2[1:1000, ])
}
#export dataset
write.csv(dataset.total, "C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.to.9.csv")
#dataset.total <- read.csv("~/Desktop/ap.spark.data/final.clusters/dataset.1.to.9.csv")
#dataset.total <- dataset.total[ ,-1]
#loop dataset epochs 10 to 32
for (j in 10:32){
dataset.1 <- dataset.total
dataset.2 <- read.csv(dataset.urls[j])
for (k in 1:NROW(unique(dataset.1$subset))) {
l <- as.integer(k + 1000)
k <- as.integer(k)
dataset.1$subset[dataset.1$subset == k] <- dataset.2$subset[l]
}
dataset.total <- rbind(dataset.1, dataset.2[1:1000, ])
}
#export dataset 1 to 32
write.csv(dataset.total, "C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.to.32.csv")
#dataset.total <- read.csv("~/Desktop/ap.spark.data/final.clusters/dataset.1.to.32.csv")
#dataset.total <- dataset.total[ , -1]
#final epoch 33 to 34
dataset.1 <- dataset.total
dataset.2 <- read.csv(dataset.urls[33])
dataset.subset.1 <- subset(dataset.total,
dataset.total$subset == 7 | dataset.total$subset == 8 | dataset.total$subset == 9)
dataset.subset.2<- subset(dataset.total,
dataset.total$subset == 3 | dataset.total$subset == 4)
dataset.subset.3<-subset(dataset.total,
dataset.total$subset == 1)
dataset.subset.4<-subset(dataset.total,
dataset.total$subset == 2)
#assign of final labels
dataset.subset.1$subset <- 1
dataset.subset.2$subset <- 4
dataset.subset.3$subset <- 2
dataset.subset.4$subset <- 3
#binding of all data
dataset.total <- rbind(dataset.subset.1, dataset.subset.2, dataset.subset.3, dataset.subset.4)
dataset.total <- rbind(dataset.total, dataset.2[1:(NROW(dataset.2) - k), ])
unique(dataset.total$subset)
#export dataset from 1 to 33
write.csv(dataset.total, "C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.to.33.csv")
rm(list = ls())
#require(fossil)
cluster <- read.csv("C:/users/nikos/Desktop/ap.spark.data/final.clusters/dataset.1.to.33.csv")
cluster2 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/label_data123.txt",
header = FALSE)
cluster.1.1 <- cluster$subset[1:3000]
cluster.1.2 <- cluster$subset[3001:6000]
cluster.1.3 <- cluster$subset[6001:9000]
cluster.1.4 <- cluster$subset[9001:12000]
cluster.1.5 <- cluster$subset[12001:15000]
cluster.1.6 <- cluster$subset[15001:18000]
cluster.1.7 <- cluster$subset[18001:21000]
cluster.1.8 <- cluster$subset[21001:24000]
cluster.1.9 <- cluster$subset[24001:27000]
cluster.1.10 <- cluster$subset[27001:30000]
cluster.1.11 <- cluster$subset[30001:32256]
cluster.2.1 <- cluster2[1:3000, ]
cluster.2.2 <- cluster2[3001:6000, ]
cluster.2.3 <- cluster2[6001:9000, ]
cluster.2.4 <- cluster2[9001:12000, ]
cluster.2.5 <- cluster2[12001:15000, ]
cluster.2.6 <- cluster2[15001:18000, ]
cluster.2.7 <- cluster2[18001:21000, ]
cluster.2.8 <- cluster2[21001:24000, ]
cluster.2.9 <- cluster2[24001:27000, ]
cluster.2.10 <- cluster2[27001:30000, ]
cluster.2.11 <- cluster2[30001:32256, ]
require(fossil)
rand.index(cluster.1.1, cluster.2.1)
rand.index(cluster.1.2, cluster.2.2)
rand.index(cluster.1.3, cluster.2.3)
rand.index(cluster.1.4, cluster.2.4)
rand.index(cluster.1.5, cluster.2.5)
rand.index(cluster.1.6, cluster.2.6)
rand.index(cluster.1.7, cluster.2.7)
rand.index(cluster.1.7, cluster.2.8)
rand.index(cluster.1.9, cluster.2.9)
rand.index(cluster.1.10, cluster.2.10)
rand.index(cluster.1.11, cluster.2.11)
mean.rand.index <- (rand.index(cluster.1.1, cluster.2.1) +
rand.index(cluster.1.2, cluster.2.2) +
rand.index(cluster.1.3, cluster.2.3) +
rand.index(cluster.1.4, cluster.2.4) +
rand.index(cluster.1.5, cluster.2.5) +
rand.index(cluster.1.6, cluster.2.6) +
rand.index(cluster.1.7, cluster.2.7) +
rand.index(cluster.1.7, cluster.2.8) +
rand.index(cluster.1.9, cluster.2.9) +
rand.index(cluster.1.10, cluster.2.10)+
(2256/3000) * rand.index(cluster.1.11, cluster.2.11)
) / (10 + (2256 / 3000))
mean.rand.index
adj.rand.index(cluster.1.1, cluster.2.1)
adj.rand.index(cluster.1.2, cluster.2.2)
adj.rand.index(cluster.1.3, cluster.2.3)
adj.rand.index(cluster.1.4, cluster.2.4)
adj.rand.index(cluster.1.5, cluster.2.5)
adj.rand.index(cluster.1.6, cluster.2.6)
adj.rand.index(cluster.1.7, cluster.2.7)
adj.rand.index(cluster.1.8, cluster.2.8)
adj.rand.index(cluster.1.9, cluster.2.9)
adj.rand.index(cluster.1.10, cluster.2.10)
adj.rand.index(cluster.1.11, cluster.2.11)
mean.adj.rand.index <- (adj.rand.index(cluster.1.1, cluster.2.1) +
adj.rand.index(cluster.1.2, cluster.2.2) +
adj.rand.index(cluster.1.3, cluster.2.3) +
adj.rand.index(cluster.1.4, cluster.2.4) +
adj.rand.index(cluster.1.5, cluster.2.5) +
adj.rand.index(cluster.1.6, cluster.2.6) +
adj.rand.index(cluster.1.7, cluster.2.7) +
adj.rand.index(cluster.1.8, cluster.2.8) +
adj.rand.index(cluster.1.9, cluster.2.9) +
adj.rand.index(cluster.1.10, cluster.2.10) +
(2256/3000) * adj.rand.index(cluster.1.11, cluster.2.11)
) / (10 + (2256/3000))
cat("Average of rand index is: ", mean.rand.index, "\n")
cat("Average of adjusted Rand index is: ", mean.adj.rand.index)
data.1 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/Dataset/data1.txt", sep = "\t")
data.1 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/data1.txt", sep = "\t")
data.2 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/data2.txt", sep = "\t")
data.3 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/data3.txt", sep = "\t")
names(data.1) <- names(data.2)
names(data.3) <- names(data.2)
data <- rbind(data.1, data.2, data.3)
require(apcluster)
#graphs loop
for (y in 1:33){
k = y * 1000
l = k -1000 + 1
if (k > NROW(data)){
k <- NROW(data)
}
data.subset <- data[l:k, ]
sim <- negDistMat(data.subset, r = 2)
exemplar.cluster.0 <- apcluster(sim, details = TRUE, q = 0.0229)
#creation of graph of the first subset
plot(exemplar.cluster.0, data.subset)
plot.file.path <- paste("C:/users/nikos/Desktop/ap.spark.data/graphs.without.weights/", y, ".png")
plot.file.path <- gsub(" ", "", plot.file.path)
plot.file.path
png(filename=plot.file.path)
plot(exemplar.cluster.0, data.subset)
dev.off()
}
rm(list = ls())
require("sparklyr")
require("future")
require(SparkR)
setwd("C:/users/nikos/Desktop/ap.spark.data/")
sc <- spark_connect(master = "local", spark_version = "2.4.3")
if(file.exists("source")) unlink("source", TRUE)
if(file.exists("source-out")) unlink("source-out", TRUE)
#AP_affinity_propagation(data, p, maxits = 1000, convits = 100,
#                        dampfact = 0.9, details = FALSE, nonoise = 0, time = FALSE)
stream_generate_test(iterations = 1)
read_folder <- stream_read_csv(sc, "./datasets/")
write_output <- stream_write_csv(read_folder, "source-out")
#invisible(future(stream_generate_test(interval = 0.5)))
rm(list = ls())
require(apcluster)
#require(phyclust)
#load of data.1
data.1 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/mixed.data/data1.txt", sep = "\t")
data.2 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/mixed.data/data2.txt", sep = "\t")
data.3 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/mixed.data/data3.txt", sep = "\t")
names(data.1) <- names(data.2)
names(data.3) <- names(data.2)
total_time<-0
start_time <- Sys.time()
data <- rbind(data.1, data.2, data.3)
#data <- read.csv("C:/users/nikos/Desktop/ap.spark.data/data1.txt", sep = "\t")
#subset of the first 1000 rows
loop = round(NROW(data) / 1000 )
if (NROW(data)/1000 - round(NROW(data)/1000) < 0.51){
loop <- loop + 1
}else {}
#loop
for (y in 1:loop) {
k = y * 1000
l = k -1000 + 1
if (k > NROW(data)){
k <- NROW(data)
}
data.subset <- data[l:k, ]
if (exists("vector.for.next.epoch")){
vector.for.next.epoch <- as.data.frame(vector.for.next.epoch)
names(vector.for.next.epoch) <- c(names(data.subset)[1], names(data.subset)[2])
data.subset <- rbind(data.subset, vector.for.next.epoch)
f.path <- paste0("C:/users/nikos/Desktop/ap.spark.data/to.be.clustered/dataset", y)
write.csv(data.subset, f.path)
}else{
f.path <- paste0("C:/users/nikos/Desktop/ap.spark.data/to.be.clustered/dataset", y)
write.csv(data.subset, f.path)
}
#estimation of similarity
sim <- negDistMat(data.subset, r = 2)
#affinity propagation
#p.value = round(sum(data.subset) / 2000)
#p.value
exemplar.cluster.0 <- apcluster(sim, details = TRUE, q = 0.0229)
exemplar.cluster.0
#storage of exemplars of the subset
exemplars.epoch.0 <- exemplar.cluster.0@exemplars
#creation of a dataframe that has the values, the exemplars and the subset indication
total.info <- as.data.frame(matrix(nrow = 0, ncol = 3))
names(total.info) <- c("values", "exemplar", "subset")
#appending in the dataframe values, exemplars and index
for(i in 1:NROW(exemplar.cluster.0@clusters)){
processed.subset <- as.data.frame(matrix(nrow = 0, ncol = 1))
names(processed.subset) <- c("values")
processed.subset <- rbind(processed.subset, exemplar.cluster.0@clusters[i])
processed.subset$exemplar <- exemplar.cluster.0@exemplars[i]
processed.subset$subset <- i
names(processed.subset) <- c("values", "exemplar", "subset")
names(total.info) <- c("values", "exemplar", "subset")
total.info <- rbind(total.info, processed.subset)
rm(processed.subset)
}
#just some calculations for adjusting alphabetical order between the shape files
wy = y
if (wy < 10){
wy <-paste0("0",wy)
}
file.to.stream.path <- paste("C:/users/nikos/Desktop/ap.spark.data/datasets/", wy, ".csv")
file.to.stream.path <- gsub(" ", "", file.to.stream.path)
write.csv(total.info, file.to.stream.path)
#creation of graph of the first subset
plot(exemplar.cluster.0, data.subset)
plot.file.path <- paste("C:/users/nikos/Desktop/ap.spark.data/graphs/", wy, ".png")
plot.file.path <- gsub(" ", "", plot.file.path)
plot.file.path
png(filename=plot.file.path)
plot(exemplar.cluster.0, data.subset)
dev.off()
#weighted.exemplars
t <- y - (y-1) - 1
w <- 2^(-0.25*(t))
weighted.exemplars <- w*exemplar.cluster.0@exemplars
#number of values clustered in each exemplar
c <- NA
for (i in 1:NROW(exemplar.cluster.0@clusters)){
d <- summary(exemplar.cluster.0@clusters[i])[1]
c <- rbind(c, d)
c <- c[-1]
}
c <- as.numeric(c)
c <- c* w
#c contains all the weights for the exemplars of the first 1000 rows
#cd contains all weights for each exemplar and each exemplar
cd <- cbind(c, exemplar.cluster.0@exemplars)
#a contains the weights multiplied by the exemplars
a <- as.numeric(cd[ ,1] * cd[ ,2])
#  a <- cd[ ,1] * cd[ ,2]
#calculate disimilarity for each exemplar's group of values
#calculation of the vector that is binded in the next subset of data
vector.for.next.epoch <- matrix(nrow = 0, ncol = 2)
for (i in 1:NROW(exemplar.cluster.0@clusters)){
f <- as.data.frame(exemplar.cluster.0@clusters[i])
sumD <- sum(-negDistMat(f, r = 2))
c <- as.array(a[i], sumD)
z <- cbind(a[i], sumD)
vector.for.next.epoch <- rbind(vector.for.next.epoch, z)
#filepath = paste("~/Desktop/ap.spark.data/vectors/ap.vector.for.next.epoch",
#                 y, ".csv")
#  filepath = gsub(" ", "", filepath)
# write.csv(vector.for.next.epoch, filepath)
}
qw = y
if (qw < 10){
qw <- paste0("0",qw)
}
filepath = paste("C:/users/nikos/Desktop/ap.spark.data/vectors/vector.for.next.epoch", qw, ".csv")
filepath = gsub(" ", "", filepath)
write.csv(vector.for.next.epoch, filepath)
end_time <- Sys.time()
time.taken <- end_time - start_time
total_time<- total_time + time.taken
cat("Time spent\n", time.taken, "\n" )
start_time=Sys.time()
end_time=Sys.time()
}
cat("Time spent for system\n", total_time, "\n" )
list <- as.data.frame(exemplar.cluster.0@exemplars)
list <- rownames(list)
cat("final exemplars are:", list)
data.1 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/mixed.data/data1.txt", sep = "\t")
data.2 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/mixed.data/data2.txt", sep = "\t")
data.3 <- read.csv("C:/users/nikos/Desktop/ap.spark.data/mixed.data/data3.txt", sep = "\t")
names(data.1) <- names(data.2)
names(data.3) <- names(data.2)
data <- rbind(data.1, data.2, data.3)
require(apcluster)
#graphs loop
for (y in 1:33){
k = y * 1000
l = k -1000 + 1
if (k > NROW(data)){
k <- NROW(data)
}
data.subset <- data[l:k, ]
sim <- negDistMat(data.subset, r = 2)
exemplar.cluster.0 <- apcluster(sim, details = TRUE, q = 0.0229)
#creation of graph of the first subset
plot(exemplar.cluster.0, data.subset)
plot.file.path <- paste("C:/users/nikos/Desktop/ap.spark.data/graphs.without.weights/", y, ".png")
plot.file.path <- gsub(" ", "", plot.file.path)
plot.file.path
png(filename=plot.file.path)
plot(exemplar.cluster.0, data.subset)
dev.off()
}
